{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4626e955",
   "metadata": {},
   "source": [
    "# Encrypted RAG with ES2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aae95e9",
   "metadata": {},
   "source": [
    "In this tutorial, we will walk through the steps to use the ES2 SDK for Encrypted Retrieval-Augmented Generation (Encrypted RAG) using fully homomorphic encryption (FHE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e16ee0a",
   "metadata": {},
   "source": [
    "## Import ES2\n",
    "\n",
    "To use the ES2 SDK, you need to install it first. Before installing, make sure you have conda installed on your system. For more details, see `SDK installation` section in `Get Started`. After installation, you can import the ES2 SDK in your Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e01311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install es2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346ddd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import es2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae05f72",
   "metadata": {},
   "source": [
    "## Initialize ES2\n",
    "\n",
    "To use the ES2 service, initialization is required. \n",
    "\n",
    "Initialization step includes 1) establishing a connection to the ES2 server, 2) configuring Crypto settings necessary for vector search, and 3) registering evaluation keys for enabling ES2 server to perform secure operations.\n",
    "\n",
    "You can set the path and ID of the key for data encryption, presets for operations, query encryption, database encryption, and index type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a29819",
   "metadata": {},
   "outputs": [],
   "source": [
    "es2.init(\n",
    "    host=\"localhost\",\n",
    "    port=50050,\n",
    "    key_path=\"./keys\",\n",
    "    key_id=\"rag_key_id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb306a5",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "### Prepare Plaintext Vectors\n",
    "\n",
    "To perform RAG, we need to prepare the plaintext text embedding vectors. Note that these vectors should be normalized for identification metric, cosine similarity. This is just one example of text embedding; you can also use your own embedding model to generate vectors from your text dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca634a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94879807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-***\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27a559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# embedding function\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    embedding = client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "    vec = np.array(embedding, dtype=np.float32)\n",
    "    norm = np.linalg.norm(vec)\n",
    "    if norm != 0:\n",
    "        vec /= norm\n",
    "    return vec\n",
    "\n",
    "# Prepare vectors to be indexed\n",
    "db_text = [\n",
    "    \"The capital of France is Paris.\",\n",
    "    \"The capital of Germany is Berlin.\",\n",
    "    \"The capital of Italy is Rome.\",\n",
    "    \"The capital of Canada is Ottawa.\",\n",
    "    \"The capital of South Korea is Seoul.\",\n",
    "]\n",
    "\n",
    "# get embeddings\n",
    "db_vectors = np.stack([get_embedding(txt) for txt in db_text])\n",
    "dim = db_vectors.shape[1]\n",
    "\n",
    "print(f\"Vector Dimension: {dim}\")\n",
    "print(f\"Number of Vectors: {db_vectors.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f660d6",
   "metadata": {},
   "source": [
    "## Create Index and Insert Data\n",
    "\n",
    "For encrypted similarity search, we first prepare a vector index, called `Index`, to store encrypted vectors and their metadata in the ES2 system.\n",
    "An index is defined by its name and the dimensionality of the vectors it will store.\n",
    "The dimensionality must match the size of the vectors you plan to insert.\n",
    "This step ensures the database is properly configured to handle your data.\n",
    "\n",
    "If the index is ready, you can insert data into it.\n",
    "This first **encrypts the vectors** using the generated encryption keys and **inserts** them into the index in the created ES2.\n",
    "The data to be inserted can be in the form of vectors and associated metadata. \n",
    "The metadata can provide additional context or information about the vectors, such as their source or relevance.\n",
    "Each vector should match the dimensionality specified during index creation.\n",
    "\n",
    "Additionally, metadata can be attached to each vector to provide context or additional information. \n",
    "This step is essential for RAG.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552c0218",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = es2.create_index(\"rag_index\", dim=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e78233",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.insert(db_vectors, metadata=db_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60761faf",
   "metadata": {},
   "source": [
    "## Encrypted Similarity Search\n",
    "\n",
    "### Prepare query\n",
    "\n",
    "First, prepare query for encrypted search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43de7c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"What is the capital of France?\"\n",
    "\n",
    "query_vector = get_embedding(query_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b07e64e",
   "metadata": {},
   "source": [
    "### Encrypted search on the index\n",
    "\n",
    "Let's perform encrypted similarity search for encrypted RAG. \n",
    "\n",
    "Once all the encrypted vector index and encrypted query vectors are ready, we can now perform a similarity search on encrypted data without decrypting the data.\n",
    "The `index` object contains the decryption key, enabling the ES2 server to return encrypted scores. \n",
    "These scores are decrypted by the client to retrieve the top-`k` relevant results along with their indices.\n",
    "After identifying the indices by decryption and top-k selection, we retrieve the encrypted documents and decrypt them to obtain the plaintext.\n",
    "\n",
    "This process ensures secure and efficient similarity search operations, even when working with encrypted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a5e6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = index.search(query_vector, top_k=1, output_fields=[\"metadata\"])[0]\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4b8d0a",
   "metadata": {},
   "source": [
    "### Generate Answers with Retrieval-augmented Context\n",
    "\n",
    "Once the decrypted documents are retrieved, we can use LLM (e.g. OpenAI's GPT) to generate answers based on the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0f2f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = [res[\"metadata\"] for res in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dccb5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(docs, query, model=\"gpt-4\"):\n",
    "    instruction = \"You are an assistant that answers questions based on the provided documents.\"\n",
    "    prompt = f\"\"\"{instruction}:\\n\\n[Documents]\\n\"\"\"\n",
    "    for doc in docs:\n",
    "        prompt += f\"- {doc}\\n\"\n",
    "    prompt += f\"\\n[Question]\\n{query}\\n[Answer]\\n\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,  # Chat model\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": instruction},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=128,\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "answer = generate_answer(retrieved_docs, query_text)\n",
    "print(f\"Generated Answer: \\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9455f87",
   "metadata": {},
   "source": [
    "### Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684a8a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "es2.drop_index(\"rag_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8807c5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "es2.release_key(\"rag_key_id\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
