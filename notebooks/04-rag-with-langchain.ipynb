{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4626e955",
   "metadata": {},
   "source": [
    "# Encrypted RAG with LangChain-enVector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aae95e9",
   "metadata": {},
   "source": [
    "This example demonstrates the complete workflow of the enVector Python SDK, showcasing its capabilities for Encrypted Retrieval-Augmented Generation (Encrypted RAG) using fully homomorphic encryption (FHE). \n",
    "In this example, we'll see:\n",
    "\n",
    "- How text data is stored and encrypted in the index for RAG\n",
    "- How the encrypted similarity search is performed with FHE\n",
    "- How the LLM (Ollama using ChatGPT OSS) leverages RAG while keeping results encrypted until decryption \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e16ee0a",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- enVector server reachable from this notebook environment\n",
    "- Registered key path and key ID for the target index\n",
    "- `es2`, `langchain`, `langchain-community`, `langchain-text-splitters`, and `sentence-transformers` packages installed\n",
    "- A PDF document accessible from the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa1492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall langchain-envector -y\n",
    "# !pip install langchain-envector==0.1.2 --force-reinstall\n",
    "# !pip install langchain-community --force-reinstall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13071400",
   "metadata": {},
   "source": [
    "### Import langchain-envector\n",
    "\n",
    "Import `langchain_envector` to use enVector with the LangChain framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c6d298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_envector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5e10a9",
   "metadata": {},
   "source": [
    "First, load a sample document to search.\n",
    "\n",
    "In this example, we use a NIST report. This report evaluates how accurate and reliable common empirical formulas are when used to predict fire behavior in various scenarios. For more details about the report, see [NIST Report](https://www.nist.gov/publications/verification-and-validation-commonly-used-empirical-correlations-fire-scenarios) and download the PDF from [Link](https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1169.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0bcbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "PDF_PATH = Path(\"./NIST.SP.1169.pdf\")  # Update with your PDF path\n",
    "assert PDF_PATH.exists(), f\"PDF file not found: {PDF_PATH}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aee94b1",
   "metadata": {},
   "source": [
    "## Configure connection and embeddings\n",
    "Fill in your enVector connection details and choose an embedding model. The model dimension must match the index dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4141e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ES2_ADDRESS = os.getenv(\"ES2_ADDRESS\", \"0.0.0.0:50050\")\n",
    "ES2_ACCESS_TOKEN = os.getenv(\"ES2_ACCESS_TOKEN\", \"\")\n",
    "ES2_KEY_PATH = os.getenv(\"ES2_KEY_PATH\", \"./keys\")\n",
    "ES2_KEY_ID = os.getenv(\"ES2_KEY_ID\", \"example_key\")\n",
    "INDEX_NAME = os.getenv(\"ES2_INDEX_NAME\", \"pdf_demo\")\n",
    "EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "print(f\"Using key '{ES2_KEY_ID}'\")\n",
    "print(f\"Using index '{INDEX_NAME}'\")\n",
    "print(f\"Embedding model: {EMBEDDING_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03d683f",
   "metadata": {},
   "source": [
    "## Load the PDF and split into chunks\n",
    "We rely on LangChain community loaders and text splitters to turn the PDF pages into retrieval-friendly passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d722f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = PyPDFLoader(str(PDF_PATH))\n",
    "raw_docs = loader.load()\n",
    "print(f\"Loaded {len(raw_docs)} pages from {PDF_PATH.name}\")\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=900, chunk_overlap=150)\n",
    "chunked_docs = splitter.split_documents(raw_docs)\n",
    "print(f\"Created {len(chunked_docs)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdee56cd",
   "metadata": {},
   "source": [
    "## Prepare text and metadata payloads\n",
    "enVector expects parallel lists of texts and metadata dictionaries. Here we keep track of the original page number for traceability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1d85c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "metadatas = []\n",
    "for doc in chunked_docs:\n",
    "    texts.append(doc.page_content)\n",
    "    meta = dict(doc.metadata)\n",
    "    meta.setdefault(\"source\", PDF_PATH.name)\n",
    "    metadatas.append(meta)\n",
    "\n",
    "print(texts[0][:200])\n",
    "print(metadatas[0])\n",
    "print(f\"Prepared {len(texts)} text chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838edd91",
   "metadata": {},
   "source": [
    "## Set embedding model\n",
    "\n",
    "We'll use HuggingFace embeddings to convert our text chunks into numerical vectors that can be encrypted and searched.\n",
    "The embeddings model will transform each text chunk into a high-dimensional vector that captures semantic meaning.\n",
    "These vectors will be encrypted before being stored in the enVector index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab7fec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "vector_dim = len(embeddings.embed_query(\"envector probe\"))\n",
    "print(f\"Embedding dimension: {vector_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae05f72",
   "metadata": {},
   "source": [
    "## Initialize the enVector store\n",
    "Configure the encrypted vector index and instantiate the LangChain-compatible store. The embedding model derives the vector dimension automatically.\n",
    "\n",
    "Initialization step includes:\n",
    "1. `ConnectionConfig`: establishing a connection to the enVector server, \n",
    "2. `IndexSettings`: configuring index settings necessary for vector search, including query and metadata encryption, and\n",
    "3. `KeyConfig`: registering evaluation keys to enable the enVector server to perform secure operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b6d8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_envector.config import ConnectionConfig, EnvectorConfig, IndexSettings, KeyConfig\n",
    "from langchain_envector.vectorstore import Envector\n",
    "\n",
    "config = EnvectorConfig(\n",
    "    connection=ConnectionConfig(address=ES2_ADDRESS, access_token=ES2_ACCESS_TOKEN) if ES2_ACCESS_TOKEN else ConnectionConfig(address=ES2_ADDRESS),\n",
    "    key=KeyConfig(key_path=ES2_KEY_PATH, key_id=ES2_KEY_ID, preset=\"ip\", eval_mode=\"rmp\"),\n",
    "    index=IndexSettings(index_name=INDEX_NAME, dim=vector_dim, query_encryption=\"plain\"),\n",
    "    create_if_missing=True,\n",
    ")\n",
    "store = Envector(config=config, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0786138c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List envector key files\n",
    "!ls -l $ES2_KEY_PATH/$ES2_KEY_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb306a5",
   "metadata": {},
   "source": [
    "## Insert chunks (batched)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef21cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = store.add_texts(texts, metadatas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347b68a0",
   "metadata": {},
   "source": [
    "### Encrypted search on the index\n",
    "\n",
    "Let's perform an encrypted similarity search using LangChain-enVector.\n",
    "\n",
    "The enVector vectorstore provides a simple interface through LangChain to perform similarity search on encrypted data.\n",
    "Under the hood, enVector handles all the encryption, decryption, and secure search operations automatically.\n",
    "When we call `similarity_search()`, the query is encrypted, the secure similarity search is performed on the encrypted vectors,\n",
    "and the results are decrypted before being returned.\n",
    "\n",
    "The `store.similarity_search()` method returns the top-k most relevant documents along with their similarity scores,\n",
    "making it easy to build secure RAG applications without having to manage encryption directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3947f802",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Which organizations collaborated on NIST SP 1169â€™s fire model verification and validation study, and what larger NRC report summarizes the results?\"\n",
    "\n",
    "# Query in plaintext\n",
    "results = store.similarity_search(query, k=3)\n",
    "for idx, doc in enumerate(results, start=1):\n",
    "    print(f\"--- Result {idx} (score={doc.metadata.get('_score'):.4f}) ---\")\n",
    "    print(doc.page_content[:400], \"...\")\n",
    "    print({k: v for k, v in doc.metadata.items() if not k.startswith('_')})\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4b8d0a",
   "metadata": {},
   "source": [
    "### Generate Answers with Retrieval-augmented Context\n",
    "\n",
    "Once the decrypted documents are retrieved, we can use an LLM (e.g. OpenAI's GPT) to generate answers based on the retrieved documents.\n",
    "\n",
    "In this example, we use the gpt-oss model running locally with ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0163555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def generate_answer(docs, query, model=\"gpt-oss\"):\n",
    "    instruction = \"You are an assistant that answers questions based on the provided documents.\"\n",
    "    prompt = f\"\"\"{instruction}:\\n\\n[Documents]\\n\"\"\"\n",
    "    for doc in docs:\n",
    "        prompt += f\"- {doc}\\n\"\n",
    "    prompt += f\"\\n[Question]\\n{query}\\n[Answer]\\n\"\n",
    "\n",
    "    response = requests.post(\n",
    "        \"http://localhost:11434/api/chat\",\n",
    "        json={\n",
    "            \"model\": model,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": instruction},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            \"stream\": False\n",
    "        }\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"message\"][\"content\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a9b019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "answer = generate_answer(results[0].metadata, query)\n",
    "print(f\"Generated Answer: \\n{answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "es2-sdk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
